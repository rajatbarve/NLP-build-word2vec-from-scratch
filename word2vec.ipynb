{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac403f49-9ff1-4fe5-957d-e537e67a262d",
   "metadata": {},
   "source": [
    "# Manual Implementation of Word Embeddings Using CBOW\n",
    "\n",
    "In this (fairly bare-bones) implementation, we manually create word embeddings using the **Continuous Bag of Words (CBOW)** method.  \n",
    "While libraries like `gensim` or pre-trained models like `Word2Vec` and `BERT` can do this with a single line of code, our goal here is to build a foundational understanding of how word embeddings are created from scratch.  \n",
    "\n",
    "The idea is to focus on the core mechanics of CBOW while skipping advanced optimizations for now.  \n",
    "By the end of this notebook, we'll have a clearer picture of how word vectors are generated and how they represent semantic relationships.  \n",
    "\n",
    "---\n",
    "\n",
    "## Key Highlights  \n",
    "\n",
    "1. **Custom Implementation**:  \n",
    "   - This implementation does not use pre-built deep learning modules. It’s intentionally basic to show what’s happening under the hood.  \n",
    "\n",
    "2. **Focus on Fundamentals**:  \n",
    "   - The goal is to grasp the mechanics of CBOW, not to optimize performance or accuracy.  \n",
    "\n",
    "3. **Simplified Pipeline**:  \n",
    "   - Hyperparameter tuning, rigorous preprocessing, and advanced evaluation techniques have been skipped intentionally to keep the focus on the essentials.\n",
    "\n",
    "4. **CUDA Not Invoked**:  \n",
    "   - Since our dataset is quite small (vocabulary of ~14,000 words) relative to truly large language models (LLMs), CUDA is not invoked for acceleration. \n",
    "\n",
    "---\n",
    "\n",
    "## Scope for Future Improvement  \n",
    "\n",
    "1. **Hyperparameter Tuning**:  \n",
    "   - Experiment with different values for learning rate, hidden layer size, and epochs to improve performance.  \n",
    "   - Use techniques like grid search or random search for systematic tuning.  \n",
    "\n",
    "2. **Improved Preprocessing**:  \n",
    "   - Apply advanced lemmatization techniques for better word normalization.  \n",
    "   - Use more sophisticated tokenization to handle contractions, punctuation, and multi-word expressions.  \n",
    "\n",
    "3. **Testing Quality of Embeddings**:  \n",
    "   - Evaluate embeddings using downstream tasks like **sentiment analysis**, clustering, or word analogies.  \n",
    "   - Compare results against advanced models like **RoBERTa** or **BERT** for benchmarking.  \n",
    "\n",
    "4. **Integration with Downstream Tasks**:  \n",
    "   - Integrate embeddings into applications like recommendation systems or text classification to assess practical utility.  \n",
    "\n",
    "---\n",
    "\n",
    "## Closing Remarks  \n",
    "\n",
    "This implementation skips many important steps, like regularization, rigorous preprocessing, and hyperparameter tuning.  \n",
    "The aim is not to build a production-ready model but to provide a clear understanding of how word embedding vectors are generated.  \n",
    "\n",
    "With these fundamentals in place, we can confidently explore more advanced NLP techniques and frameworks in the future.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abd822c-5c18-4ecb-be62-235f166db9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "import nltk\n",
    "import re\n",
    "import torch\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e770a68-e683-4697-b3c5-abbe1f5de1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary nltk tools\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0109b94f-8036-4015-a90b-0a0c08af4984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read corpus to train the model on. For this exercise we select some random news articles. The data is available on Kaggle. \n",
    "\n",
    "with open(\"reddit_news_headlines.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "# game_of_thrones1_graft.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809d3ae9-54ed-400b-a554-940f4cab1b2c",
   "metadata": {},
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a967f-1b03-4659-85a9-156e91ca43f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We follow the following steps to preprocess data. Ideally, we would also include more rigorous cleaning methods\n",
    "# like removing emojis and words from other languages etc. But since we are working with a simple corpus, these steps are not necessary.\n",
    "\n",
    "# Preprocess 1\n",
    "\n",
    "\"\"\"\n",
    "1. Lower case\n",
    "2. Remove punctuation, special characters, numbers\n",
    "2. Remove stopwords/punctuation\n",
    "3. Tokenize and Stemming\n",
    "\"\"\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "raw_text = raw_text.lower()\n",
    "raw_text = re.sub(r\"[^a-zA-Z\\s.]\", \"\", raw_text)\n",
    "raw_text = re.sub(r\"'s\\b\", \"\", raw_text)  # Remove occurrences of \"'s\"\n",
    "# raw_text = re.sub(r\"([\\\\\\[\\]\\{\\},__;!?/*^@'\\(\\)\\:\\'\\\"\\`$&\\%’“”-_])|(\\n)|(\\d+)\", \"\", raw_text)\n",
    "raw_text = re.sub(r\"-\", \" \", raw_text)\n",
    "\n",
    "raw_text = re.sub(r\"\\s+\\.\", \".\", raw_text)\n",
    "\n",
    "# filter_text = \" \".join(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebe3653-76af-40d9-a450-60023b4f9c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEMMATIZE WORDS\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Function to lemmatize the word\n",
    "def lemmatize_word(x):\n",
    "    pos_tags = nltk.pos_tag([x])\n",
    "    lemmatized_word = [lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "    stemmed = [stemmer.stem(word) for word in lemmatized_word]\n",
    "    return lemmatized_word[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3b5cc1-2f91-4aad-8420-24e914308646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess 2\n",
    "\"\"\"\n",
    "Lemmatizing words handles cases like 'drawing' and 'drawn' reduced to 'draw'\n",
    "\"\"\"\n",
    "words = word_tokenize(raw_text)\n",
    "words_lemmatized = [lemmatize_word(word) for word in words]\n",
    "filter_text = \" \".join(words_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f23677-bb76-4f4e-b6d2-5a03cf59c601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess 3\n",
    "\"\"\"\n",
    "Remove stop_words.\n",
    "Remove whitespace between period and preceding word.\n",
    "Replace multiple periods with a single period.\n",
    "Introduce space between period and following word if there is none.\n",
    "\"\"\"\n",
    "filter_text = word_tokenize(filter_text)\n",
    "filter_text = \" \".join([word for word in filter_text if word not in stop_words])\n",
    "\n",
    "filter_text = filter_text.replace(\" .\",\".\")\n",
    "filter_text = re.sub(r'\\.{2,}', '.', filter_text)\n",
    "filter_text = re.sub(r'\\.(?=\\S)', '. ', filter_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c223aba-1b25-4da4-a875-ec403ea13171",
   "metadata": {},
   "source": [
    "### Vocabulary Creation and One-Hot Encoding\n",
    "\n",
    "In this step, we create a **vocabulary** from our text corpus. A vocabulary consists of unique words, which form the foundation for many **Natural Language Processing (NLP)** tasks. \n",
    "\n",
    "Next, we generate a **repository** to store the **one-hot encoded** representations of each word in the vocabulary. This allows us to represent each word as a vector, where each word in the vocabulary is associated with a unique index and is represented by a vector of zeros, except for the position corresponding to the word, which is set to one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d57bc01-fde0-4f8d-a500-e82a7933bb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Corpus words: {len([word.strip() for word in re.split(r'[ .]+', filter_text)])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355dda2e-5e21-4694-be99-42d383aacabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(word.strip() for word in re.split(r'[ .]+', filter_text) if word.strip())\n",
    "vocab = {word for word in vocab if word not in stop_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefd6800-065d-4e98-84dd-8080ca845c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_encoding = pd.DataFrame(vocab)\n",
    "words_encoding.columns = [\"vocab\"]\n",
    "words_encoding[\"vocab\"] = words_encoding[\"vocab\"].str.replace(\".\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f8c336-8e2e-422f-856f-5c5e9912fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_encoding = pd.get_dummies(words_encoding).astype(int)\n",
    "words_encoding.columns = [col.split('_')[1] for col in words_encoding.columns] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decbe311-ecb8-4515-b387-e7d120f1f8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reorder_columns = words_encoding.idxmax(axis=1).values\n",
    "words_encoding = words_encoding[reorder_columns]\n",
    "words_encoding.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da6b17e-bb37-4bfd-8a7c-6e049132ed67",
   "metadata": {},
   "source": [
    "### Convert Text to Input Ready for Neural Network\n",
    "\n",
    "In the following steps, we will transform the raw text into a format that is suitable for input into a neural network, specifically for training the **Continuous Bag of Words (CBOW)** algorithm.\n",
    "\n",
    "#### **CBOW Overview**\n",
    "\n",
    "The **CBOW** model works by predicting a **focus word** using a window of surrounding **context words**. The size of this window is a hyperparameter that can be adjusted. For this exercise, we will consider five context words on each side of the focus word, resulting in a total of eight context words.\n",
    "\n",
    "#### **Objective**\n",
    "\n",
    "The objective of this process is to generate two primary columns:\n",
    "\n",
    "1. **Target Word**: This column will contain the one-hot encoded version of the focus word, referred to as the \"target.\"\n",
    "   \n",
    "2. **Context Words**: This column will contain the one-hot encoded representations of the surrounding context words. The number of context words is determined by the hyperparameter `num_neighbours`. For each focus word, we will include `2*m` context words, where `m` is the number of neighbours.\n",
    "\n",
    "By organizing the data this way, we create a structure that allows us to train the CBOW model effectively, using the target word and its context words to learn meaningful word embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885bcdaf-e5bd-47da-a85d-791846afe0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get context words\n",
    "\n",
    "def make_windows(sentence, num_neighbours):\n",
    "    family = []\n",
    "    sentence = sentence.split()\n",
    "    sentence = [word for word in sentence if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    for idx, word in enumerate(sentence):  \n",
    "        if idx == 0:\n",
    "            x = {}\n",
    "            try:\n",
    "                x[word] = [sentence[idx + 1]]\n",
    "                family.append(x)\n",
    "                continue\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "        context_words = []\n",
    "        # Left neighbours\n",
    "        for step in range(1, num_neighbours + 1):\n",
    "            try:\n",
    "                context_words.append(sentence[idx - step])  \n",
    "            except IndexError:  # Ignore if we go out of bounds\n",
    "                pass\n",
    "        \n",
    "        # Right neighbours\n",
    "        for step in range(1, num_neighbours + 1):\n",
    "            try:\n",
    "                context_words.append(sentence[idx + step]) \n",
    "            except IndexError:  # Ignore if we go out of bounds\n",
    "                pass\n",
    "        \n",
    "        # Append the context and target word\n",
    "        x = {}\n",
    "        x[word] = context_words\n",
    "        family.append(x)\n",
    "    \n",
    "    return family\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27243c82-27f9-4e40-9696-8e0f26299370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each sentence in the text we will have a list of dictionaries. The dictionary looks as shown below:\n",
    "\"\"\"\n",
    "{\n",
    "    focus_word1: ['context_word11', 'context_word12'.......'],\n",
    "    focus_word2: ['context_word21', 'context_word22'.......'],\n",
    " }\n",
    "\"\"\"\n",
    "\n",
    "data = []\n",
    "num_neighbours = 5\n",
    "for sent in sent_tokenize(filter_text):\n",
    "    data.append(make_windows(sent, num_neighbours=num_neighbours))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86901784-6135-4f68-bb59-6e1492bea017",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [item for sublist in data for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb001e6-daa4-4fc4-a78a-043f3ec5dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame()\n",
    "data_df[\"context_words\"] = [list(item.values())[0] for item in data]\n",
    "data_df[\"target\"] = [list(dictionary.keys())[0] for dictionary in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7510f7-5990-42fd-9f8b-57fd170af247",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"context_words\"] = data_df[\"context_words\"].apply(\n",
    "    lambda row: [re.sub(r\",|\\.|\\(\\)|;\", \"\", word) for word in row]\n",
    ")\n",
    "\n",
    "data_df[\"target\"] = data_df[\"target\"].str.replace(r\",|\\.|\\(\\)|;\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61139b27-79f0-43dc-bc83-5423dd09fa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"target\"] = data_df[\"target\"].apply(lemmatize_word)\n",
    "data_df[\"context_words\"] = data_df[\"context_words\"].apply(lambda row: [lemmatize_word(word) for word in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb2ff00-16cc-419a-9b11-32da3d0beb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"num_context_words\"] = data_df[\"context_words\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c11b14a-e961-440c-adf2-9414c20b278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.query(\"num_context_words == @num_neighbours * 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be75ae9b-2497-4690-823a-c9b852a8a5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert all the words in above dataframe into their one-hot-encoded versions. \n",
    "# This is where we invoke the dataframe created previously called \"words_encoding.\"\n",
    "\n",
    "\"\"\"\n",
    "First, we check that data_df contains only valid words, i.e., no word is out of vocabulary\n",
    "\"\"\"\n",
    "valid_words = set(words_encoding.columns)\n",
    "\n",
    "def is_valid_row(row):\n",
    "    return all(word in valid_words for word in row['context_words']) and row['target'] in valid_words\n",
    "\n",
    "data_df = data_df[data_df.apply(is_valid_row, axis=1)].reset_index(drop=True)\n",
    "\n",
    "\"\"\"\n",
    "Second, return one-hot-encoded vectors for each word\n",
    "\"\"\"\n",
    "\n",
    "def word2ohe(word):\n",
    "    ohe_vector = words_encoding[word].values\n",
    "    return ohe_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300e2e19-62b3-49bb-84af-d40475ee7f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"context_words_ohe\"] = data_df[\"context_words\"].apply(lambda word_list: [word2ohe(word) for word in word_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3760f5b-d790-43da-87e1-071c0dd56946",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"target_ohe\"] = data_df[\"target\"].apply(word2ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5f3277-bf9a-4fd9-b9f6-5171a5b60ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.sample(frac=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecaf804-b878-4856-bdcd-a5b411a1c327",
   "metadata": {},
   "source": [
    "### Build Neural Network\n",
    "\n",
    "The neural network for training the **CBOW** model will consist of one hidden layer. Each input to the network will have the shape **2*m, vocab_len**, where:\n",
    "- `m` is the number of neighbours on each side of the focus word (context words).\n",
    "- `vocab_len` is the length of the one-hot encoded vectors, representing the size of the vocabulary.\n",
    "\n",
    "For this exercise, we choose a word embedding length of 40. This means that the hidden layer will also have 40 neurons to match the dimensionality of the word embeddings. The output from the hidden layer is passed through a **softmax layer**, which is used to normalize the output into probabilities.\n",
    "\n",
    "The model will be trained using **cross-entropy loss** as the loss metric, which is commonly used in classification tasks to measure the difference between predicted and true distributions.\n",
    "\n",
    "#### Important Considerations:\n",
    "- After each epoch, it is essential to set the gradients to zero using `optimizer.zero_grad()`. This step ensures that gradients do not accumulate across epochs, which would otherwise cause incorrect backpropagation and lead to issues in model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e35281-9212-4dc2-a28e-fb2ba21c111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCBOW():\n",
    "    \n",
    "    def __init__(self, X):\n",
    "        embedding_len = 40\n",
    "        vocab_len = X.shape[2] \n",
    "        self.weights = torch.rand(vocab_len, embedding_len, dtype=torch.float32, requires_grad=True)\n",
    "        self.bias = torch.zeros(embedding_len, dtype=torch.float32, requires_grad=True)\n",
    "        self.output_weights = torch.rand(embedding_len, vocab_len, dtype=torch.float32, requires_grad=True)\n",
    "   \n",
    "    def forward(self, X):\n",
    "        output = torch.matmul(X, self.weights).mean(dim=1) + self.bias\n",
    "        output_logits = torch.matmul(output, self.output_weights)\n",
    "        return torch.softmax(output_logits, dim = 1)  # shape (samples,embedding_len)\n",
    "    \n",
    "    def compute_loss(self, y_pred, y):\n",
    "        loss = -(y*torch.log(y_pred)).sum(dim=1).mean()\n",
    "        return loss\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64193a8a-59dd-4569-836f-f9049742a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = np.array([x for x in data_df[\"context_words_ohe\"].values])\n",
    "X = torch.tensor(X_np, dtype=torch.float32)\n",
    "\n",
    "y_np = np.array([x for x in data_df[\"target_ohe\"].values])\n",
    "y = torch.tensor(y_np, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616367ef-7a22-41b1-bc61-4341cb3563ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if shape of input X is correct\n",
    "\n",
    "print(f\"samples: {len(data_df)}\")\n",
    "print(f\"context words: {2*num_neighbours}\")\n",
    "print(f\"vocabulary: {len(vocab)}\")\n",
    "\n",
    "print(f\"\\nInput shape: {X.shape}\")\n",
    "print(\"\\nShape of input should be (samples, context_words, vocabulary)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2c91a5-2f9f-4ef1-80d2-a0695bcc5f99",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fcf86d-5117-42bb-a7a3-bf8b0fcdbe5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = MyCBOW(X)\n",
    "\n",
    "epochs = 5000\n",
    "learning_rate = 0.1\n",
    "track_loss = []\n",
    "for epoch in range(epochs):\n",
    "    y_pred = model.forward(X)\n",
    "    loss = model.compute_loss(y_pred ,y)\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.weights -= learning_rate * model.weights.grad\n",
    "        model.bias -= learning_rate * model.bias.grad\n",
    "        model.output_weights -= learning_rate * model.output_weights.grad\n",
    "    \n",
    "    model.weights.grad.zero_()\n",
    "    model.bias.grad.zero_()\n",
    "    model.output_weights.grad.zero_()\n",
    "\n",
    "    track_loss.append(loss.detach())\n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        print(f\"Epoch: {epoch}. Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14635c4f-fb86-4d8c-9928-dd23e963d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(len(track_loss)), track_loss, label=\"cross-entropy Loss\", color='#1f77b4', linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Loss\", fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "# plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.title(\"Loss Over Epochs\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e56893f-03e2-4c2c-b899-19b82f07735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's parameters (weights and bias)\n",
    "torch.save({\n",
    "    'weights': model.weights,\n",
    "    'bias': model.bias,\n",
    "    'output_weights': model.output_weights\n",
    "}, \"manual_word2vec.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750dadf4-5f81-46d6-83ad-63204ad83873",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Map the words in our vocabulary with their respective weights.\n",
    "The most crucial part is to ensure that each word is zipped with its respective weights. That is the reason why the \n",
    "columns of the words_encoding dataframe were rearranged previously. \n",
    "\n",
    "[NOTE: alternatively, word embeddings for each word can be obtained by mutliplying the weight-matrix (shape = embedding_len X vocab_len) with the \n",
    "input-matrix of one-hot-encoded vectors (shape = vocab_len x vocab_len). The resultant matrix of this operation will be a matrix of size \n",
    "(embedding_len X vocab_len) such that the columns are the word embeddings of our vocabulary. Like before, care must be taken to keep track of the\n",
    "word-index pairing in the input matrix.]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "final_embeddings = {word:embeddings for word,embeddings in zip(words_encoding[reorder_columns].columns, model.weights.detach().numpy())}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019a52ce-e034-4c3a-b2ef-74b777823cb6",
   "metadata": {},
   "source": [
    "### Evaluation \n",
    "##### [Work In Progress]\n",
    "\n",
    "There are several ways to evaluate the quality of the embeddings generated by our simple model. Some of these methods include:\n",
    "\n",
    "- **Examining Similarity**: Analyzing the top `x` most similar words for each of the `y` most frequent words in the corpus.\n",
    "- **Benchmarking Against Advanced Models**: Comparing the embeddings with those generated by state-of-the-art, off-the-shelf pre-trained models like RoBERTa, BERT, etc.\n",
    "- **Sentiment Analysis Accuracy**: Evaluating how well the embeddings perform in sentiment analysis tasks.\n",
    "\n",
    "For now, we just compare word similarities between random word-pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d0d7a0-2db1-4bd9-9ce2-e93c08a7752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = words_encoding.columns[983]\n",
    "word2 = words_encoding.columns[73]\n",
    "\n",
    "word1 = \"google\"\n",
    "word2 = \"facebook\"\n",
    "\n",
    "def cosine_similarity(word1, word2):\n",
    "    embedding1 = final_embeddings[word1]\n",
    "    embedding2 = final_embeddings[word2]\n",
    "\n",
    "    embedding1 = embedding1/np.linalg.norm(embedding1)\n",
    "    embedding2 = embedding2/np.linalg.norm(embedding2)\n",
    "    \n",
    "    dot_product = np.dot(embedding1, embedding2)\n",
    "    norm1 = np.linalg.norm(embedding1)\n",
    "    norm2 = np.linalg.norm(embedding2)\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "print(f\"{word1} and {word2}\")\n",
    "cosine_similarity(word1, word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a004e1b7-10bf-4246-ab7b-ad388b9ca50b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
